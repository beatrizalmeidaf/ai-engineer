{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88ab07e8",
   "metadata": {},
   "source": [
    "## Codificando Texto para Machine Learning\n",
    "\n",
    "Os algoritmos de Machine Learning (ML) trabalham com **n√∫meros**, n√£o com palavras. Por isso, no processamento de linguagem natural (NLP), √© necess√°rio **converter palavras em n√∫meros**.\n",
    "\n",
    "### Abordagem Ing√™nua: Dicion√°rio com N√∫meros\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "| Palavra   | N√∫mero |\n",
    "|-----------|--------|\n",
    "| aardvark  | 1      |\n",
    "| king      | 2551   |\n",
    "| queen     | 3122   |\n",
    "\n",
    "**Problema**: n√∫meros grandes causam dificuldades no treinamento do modelo (como no *gradient descent*), impedindo a converg√™ncia.\n",
    "\n",
    "### Solu√ß√£o Inicial: One-hot Encoding\n",
    "\n",
    "Cada palavra √© representada por um vetor com o tamanho do vocabul√°rio, contendo `1` apenas na posi√ß√£o correspondente √† palavra.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "| Palavra   | Vetor One-hot             |\n",
    "|-----------|---------------------------|\n",
    "| aardvark  | [1, 0, 0, ...]            |\n",
    "| king      | [0, 0, ..., 1, 0, ...]     |\n",
    "| queen     | [0, 0, ..., 0, 1, 0, ...]  |\n",
    "\n",
    "#### Desvantagens do One-hot:\n",
    "- **Sparsidade**: vetores muito grandes e com muitos zeros.\n",
    "- **Sem significado sem√¢ntico**: \"king\" e \"queen\" est√£o t√£o distantes quanto \"aardvark\".\n",
    "\n",
    "> Exemplo com vocabul√°rio de 3 palavras mostra que os vetores ficam distribu√≠dos ortogonalmente no espa√ßo 3D, sem capturar rela√ß√µes entre os termos.\n",
    "\n",
    "### Solu√ß√£o Ideal: Word Embeddings\n",
    "\n",
    "Em vez de vetores esparsos, usamos **vetores cont√≠nuos e densos**, com n√∫meros aprendidos pelo modelo.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "| Palavra   | Vetor Cont√≠nuo           |\n",
    "|-----------|--------------------------|\n",
    "| aardvark  | [0.3, 1.9, -0.4]         |\n",
    "| king      | [2.1, -0.7, 0.2]         |\n",
    "| queen     | [1.5, -1.3, 0.9]         |\n",
    "\n",
    "- Esses vetores **ocupam melhor o espa√ßo vetorial**.\n",
    "- Permitem que palavras com significados semelhantes fiquem **mais pr√≥ximas entre si**.\n",
    "- S√£o conhecidos como **embeddings**, e seus valores s√£o **aprendidos** durante o treinamento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bcc18",
   "metadata": {},
   "source": [
    "## O que s√£o Word Embeddings?\n",
    "\n",
    "Um **word embedding** √© uma representa√ß√£o num√©rica aprendida de uma palavra, na forma de um **vetor com valores reais**. O objetivo √© que palavras com significados semelhantes estejam **pr√≥ximas no espa√ßo vetorial**.\n",
    "\n",
    "- Cada palavra do vocabul√°rio √© mapeada para um vetor em um espa√ßo multidimensional.\n",
    "- Esses vetores s√£o aprendidos com base no **uso da palavra no texto** (contexto).\n",
    "- Quanto mais parecidos os contextos, mais pr√≥ximas as palavras estar√£o no espa√ßo vetorial.\n",
    "\n",
    "### Exemplo de opera√ß√£o vetorial:\n",
    "\n",
    "Se fizermos:\n",
    "\n",
    "```\n",
    "vetor(\"rei\") - vetor(\"homem\") + vetor(\"mulher\") ‚âà vetor(\"rainha\")\n",
    "```\n",
    "\n",
    "Isso mostra como o modelo aprende rela√ß√µes sem√¢nticas entre palavras.\n",
    "\n",
    "### Intui√ß√£o das Dimens√µes:\n",
    "\n",
    "- As **dimens√µes do vetor** podem ser vistas como **categorias ou atributos lingu√≠sticos**.\n",
    "- Os valores dentro do vetor indicam **o quanto a palavra se associa √†quela categoria**.\n",
    "- Ex: uma dimens√£o pode representar ‚Äúrealeza‚Äù, outra ‚Äúfeminilidade‚Äù, etc.\n",
    "\n",
    "### T√©cnicas comuns de embedding:\n",
    "\n",
    "- **Word2Vec**\n",
    "  - CBOW (Continuous Bag-of-Words)\n",
    "  - Skip-gram\n",
    "- **GloVe** (Global Vectors for Word Representation)\n",
    "\n",
    "> üìù Modelos modernos como o GPT n√£o usam Word2Vec ou GloVe diretamente. Eles utilizam **Transformers** com **aten√ß√£o autom√°tica** para aprender representa√ß√µes **contextualizadas**.\n",
    "\n",
    "---\n",
    "\n",
    "## O que √© Tokeniza√ß√£o?\n",
    "\n",
    "Antes de usar embeddings, os textos precisam ser **tokenizados**. Ou seja, o texto √© dividido em partes menores chamadas **tokens**.\n",
    "\n",
    "### Exemplo de tokeniza√ß√£o simples:\n",
    "\n",
    "Frase:  \n",
    "`\"Ei, n√£o √© um √≥timo momento para estar vivo?\"`\n",
    "\n",
    "Tokeniza√ß√£o b√°sica (por espa√ßo):  \n",
    "`[\"Ei,\", \"n√£o\", \"√©\", \"um\", \"√≥timo\", \"momento\", \"para\", \"estar\", \"vivo?\"]`\n",
    "\n",
    "**Problemas**:\n",
    "- Pontua√ß√µes ficam grudadas nas palavras (`\"Ei,\"`, `\"vivo?\"`)\n",
    "- O modelo aprende representa√ß√µes diferentes para `\"vivo\"` e `\"vivo?\"`\n",
    "\n",
    "### Tokeniza√ß√£o no GPT:\n",
    "\n",
    "Modelos como o **GPT** usam **tokenizadores inteligentes**, que dividem palavras em partes menores chamadas **subpalavras**.\n",
    "\n",
    "- Por exemplo: `\"extraordin√°rio\"` pode ser tokenizado como `[\"extra\", \"ordin\", \"√°rio\"]`\n",
    "- Isso ajuda o modelo a entender palavras desconhecidas e varia√ß√µes gramaticais."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
