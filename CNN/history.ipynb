{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0f2426",
   "metadata": {},
   "source": [
    "## **Histórico das Arquiteturas de Redes Convolucionais (CNNs)**\n",
    "\n",
    "As Redes Neurais Convolucionais (CNNs) vêm sendo utilizadas há décadas. Um exemplo clássico é a **LeNet-5**, aplicada para reconhecimento de dígitos em cheques no final da década de 1990. No entanto, foi somente com a popularização das **GPUs** que redes profundas se tornaram viáveis na prática. Desde então, as CNNs passaram a superar significativamente outras abordagens em tarefas de classificação de imagens. A seguir, revisitamos os principais marcos no desenvolvimento das arquiteturas baseadas em CNNs.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. AlexNet (2012)**\n",
    "\n",
    "A **AlexNet** marcou um ponto de virada na história do aprendizado profundo. Vencedora da competição **ImageNet 2012**, obteve uma taxa de erro top-5 de **15,3%**, superando amplamente o segundo colocado (26,2%).\n",
    "\n",
    "#### **Inovações Introduzidas:**\n",
    "- **ReLU** como função de ativação (ainda pouco conhecida na época).\n",
    "- **MaxPooling** e **Dropout** para regularização.\n",
    "- Treinamento eficiente em **GPUs** com múltiplas camadas.\n",
    "\n",
    "Apesar de não ser mais considerada estado da arte, a AlexNet é um divisor de águas na história do deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Inception / GoogLeNet (2014)**\n",
    "\n",
    "A arquitetura vencedora da ImageNet 2014 foi a **GoogLeNet**, introduzindo o revolucionário **módulo Inception**.\n",
    "\n",
    "#### **Principais Características:**\n",
    "- Combina convoluções de diferentes tamanhos (1×1, 3×3 e 5×5) em paralelo.\n",
    "- Usa convoluções **1×1 como gargalo (bottleneck)** para reduzir a dimensionalidade antes de aplicar filtros maiores.\n",
    "- Utiliza **9 módulos Inception empilhados**, formando uma rede profunda, mas eficiente.\n",
    "\n",
    "**Desempenho:** erro top-5 de **6,67%**, com apenas 7 milhões de parâmetros (menos do que os 138 milhões da VGG).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. VGG (2014)**\n",
    "\n",
    "A arquitetura **VGG**, desenvolvida pelo **Visual Geometry Group (Universidade de Oxford)**, ficou em segundo lugar na competição de 2014.\n",
    "\n",
    "#### **Características da VGG:**\n",
    "- Arquitetura simples e sequencial com pilhas de camadas convolucionais 3×3.\n",
    "- Utiliza duas grandes camadas totalmente conectadas (fully connected) antes da saída final.\n",
    "- Configurações populares incluem a **VGG-16**, com 16 camadas.\n",
    "\n",
    "**Desempenho:** erro top-5 de **8,8%**, mas com grande custo computacional (**138 milhões de parâmetros**).\n",
    "\n",
    "#### **Popularidade:**\n",
    "- Ainda amplamente utilizada por sua simplicidade.\n",
    "- Comum em aplicações de **transferência de estilo**, como transformar fotos em pinturas no estilo Van Gogh.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. ResNet (2015)**\n",
    "\n",
    "A **ResNet (Residual Network)**, criada pela Microsoft, foi a vencedora da ImageNet 2015.\n",
    "\n",
    "#### **Inovação Principal:**\n",
    "- Introdução dos **blocos residuais**, que somam a entrada original do bloco à sua saída.\n",
    "- Essa técnica combate o **desvanecimento do gradiente** em redes muito profundas.\n",
    "\n",
    "#### **Impacto:**\n",
    "- Permite o treinamento de redes com **mais de 100 camadas**.\n",
    "- **ResNet-152** alcançou **erro top-5 de 4,49%**, e modelos em ensemble chegaram a **3,57%**, superando o desempenho humano.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Outras Arquiteturas Importantes**\n",
    "\n",
    "#### **DenseNet (2016)**\n",
    "- Expande a ideia da ResNet conectando **cada camada a todas as anteriores**.\n",
    "- Permite construir redes **profundíssimas (mais de 1000 camadas)** com menos parâmetros.\n",
    "\n",
    "#### **SqueezeNet e MobileNet**\n",
    "- Focadas em **eficiência computacional**.\n",
    "- Modelos pequenos, ideais para **dispositivos móveis** com recursos limitados.\n",
    "\n",
    "#### **NASNet e PNAS (AutoML - Google)**\n",
    "- Redes **projetadas automaticamente por outras redes** (Neural Architecture Search).\n",
    "- A **NASNet** alcançou **erro top-5 de 3,8%**.\n",
    "- Essa abordagem levou à suspensão das competições ImageNet, pois os modelos superaram o desempenho humano.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusão**\n",
    "\n",
    "A evolução das CNNs mostra como o avanço computacional, somado à inovação arquitetural, transformou a visão computacional. De redes simples como a LeNet-5 até modelos gerados automaticamente como a NASNet, a trajetória revela uma busca contínua por **eficiência, profundidade e generalização**. Hoje, esses modelos são amplamente utilizados em uma variedade de aplicações, como reconhecimento facial, diagnóstico médico e carros autônomos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
